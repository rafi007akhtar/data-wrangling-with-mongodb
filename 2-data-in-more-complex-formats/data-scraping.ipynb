{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping\n",
    "\n",
    "Data is often scraped from websites. These kinds of data may be present within a `<table>` in the page.\n",
    "\n",
    "The data is going to get scrapped from [this](https://www.transtats.bts.gov/Data_Elements.aspx?Data=2) website.\n",
    "\n",
    "## Data Wranging Procedure\n",
    "For the above website in particular, that is.\n",
    "\n",
    "- Firstly, build a list of required values.\n",
    "    - Build a list of carrier values, maybe by looking at the website.\n",
    "    - Build a list of airport values, maybe by writing a script that does that.\n",
    "- Then, make HTTP requests to download all data.\n",
    "- And _then_ parse the data files.\n",
    "\n",
    "The library used for scraping this website will be [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and the following lines of code to install dependencies\n",
    "# !pip install beautifulsoup4\n",
    "# !pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import all the dependencies here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_codes(soup: BeautifulSoup, id: str):\n",
    "    '''\n",
    "    Get airport codes in a Python list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - `soup`: the `BeautifulSoup` instance containing the opened page\n",
    "    - `id`: the id of the HTML element containing the codes\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    A Python list of strings containing all the codes of the given id.\n",
    "    '''\n",
    "    codes = []\n",
    "    selector = soup.find(id=id)\n",
    "    for option in selector.find_all('option'):\n",
    "        codes.append(option['value'])\n",
    "    return codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(open(\"page_source.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carrier codes: ['All', 'AllUS', 'AllForeign', 'FL', 'AS', 'AA', 'MQ', '5Y', 'DL', 'EV', 'F9', 'HA', 'B6', 'OO', 'WN', 'NK', 'US', 'UA', 'VX']\n"
     ]
    }
   ],
   "source": [
    "carrier_codes = get_codes(soup, \"CarrierList\")\n",
    "print(f\"carrier codes: {carrier_codes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airport codes: ['All', 'AllMajors', 'ATL', 'BWI', 'BOS', 'CLT', 'MDW', 'ORD']\n"
     ]
    }
   ],
   "source": [
    "airport_codes = get_codes(soup, \"AirportList\")\n",
    "print(f\"Airport codes: {airport_codes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please note that the function 'make_request' is provided for your reference only.\n",
    "# You will not be able to to actually use it from within the Udacity web UI.\n",
    "# Your task is to process the HTML using BeautifulSoup, extract the hidden\n",
    "# form field values for \"__EVENTVALIDATION\" and \"__VIEWSTATE\" and set the appropriate\n",
    "# values in the data dictionary.\n",
    "# All your changes should be in the 'extract_data' function\n",
    "\n",
    "html_page = \"page_source.html\"\n",
    "\n",
    "\n",
    "def extract_data(page):\n",
    "    data = {\"eventvalidation\": \"\",\n",
    "            \"viewstate\": \"\"}\n",
    "    with open(page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html)\n",
    "        event_validation = soup.find(id=\"__EVENTVALIDATION\")\n",
    "        data[\"eventvalidation\"] = event_validation[\"value\"]\n",
    "        viewstate = soup.find(id=\"__VIEWSTATE\")\n",
    "        data[\"viewstate\"] = viewstate[\"value\"]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_request(data):\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "\n",
    "    s = requests.Session()\n",
    "\n",
    "    r = s.post(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "                    data={'AirportList': \"ATL\",\n",
    "                          'CarrierList': \"FL\",\n",
    "                          'Submit': 'Submit',\n",
    "                          \"__EVENTTARGET\": \"\",\n",
    "                          \"__EVENTARGUMENT\": \"\",\n",
    "                          \"__EVENTVALIDATION\": eventvalidation,\n",
    "                          \"__VIEWSTATE\": viewstate\n",
    "                    })\n",
    "\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = extract_data(html_page)\n",
    "    assert data[\"eventvalidation\"] != \"\"\n",
    "    assert data[\"eventvalidation\"].startswith(\"/wEWjAkCoIj1ng0\")\n",
    "    assert data[\"viewstate\"].startswith(\"/wEPDwUKLTI\")\n",
    "\n",
    "    \n",
    "test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Scraping\n",
    "\n",
    "**STEPS**:\n",
    "1. Look at how a browser makes requests.\n",
    "2. Emulate the same in code.\n",
    "3. If \"stuff blows up\", look at HTTP traffic.\n",
    "4. Repeat from Step 1 until it works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb28a47e64698fd316ef894d8ed3433d0649d6e25f9973778a9bc5864bd7dec0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
